{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0fdcaba-051a-41ba-8d7b-efb443511391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9978450633915856\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# Qual o valor de R² ao ajustar o modelo OLS para publicidade X preço?\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Dados de exemplo\n",
    "df = pd.DataFrame({\n",
    "    'Publicidade': [100, 200, 300, 400],\n",
    "    'Preço': [10, 16, 18, 27],\n",
    "    'Vendas': [980, 1515, 2100, 2800]\n",
    "})\n",
    "\n",
    "# Variáveis independentes\n",
    "X = df[['Publicidade', 'Preço']]\n",
    "y = df['Vendas']\n",
    "\n",
    "# Adiciona intercepto\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Ajusta o modelo\n",
    "modelo = sm.OLS(y, X).fit()\n",
    "\n",
    "# Imprime o valor de R²\n",
    "print(modelo.rsquared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1099db92-f1a3-4e1c-84ed-1fa1e652151c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1º resultado: {'userId': 1, 'id': 1, 'title': 'delectus aut autem', 'completed': False}\n",
      "Tarefas completas: 90\n",
      "Tarefas incompletas: 110\n"
     ]
    }
   ],
   "source": [
    "# importando a biblioteca requests\n",
    "import requests\n",
    "# definindo a url\n",
    "url = \"https://jsonplaceholder.typicode.com/todos\"\n",
    "# fazendo uma requisição HTTP do tipo GET para a url\n",
    "response = requests.get(url)\n",
    "# colocando a lista de resultados em uma variável\n",
    "data = response.json()\n",
    "# printando o primeiro resultado da lista\n",
    "print(\"1º resultado:\", data[0])\n",
    "# temos uma lista de dicionários\n",
    "# estamos interessados na chave 'completed'\n",
    "completed = [todo for todo in data if todo['completed'] == True]\n",
    "uncompleted = [todo for todo in data if todo['completed'] == False]\n",
    "# mostrando o número de tarefas completas e incompletas\n",
    "print(\"Tarefas completas:\", len(completed))\n",
    "print(\"Tarefas incompletas:\", len(uncompleted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e54f3d50-824c-4228-8f38-a4ed2dc9d057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O usuário publicou 10 posts.\n"
     ]
    }
   ],
   "source": [
    "#Chegou a sua vez, vamos utilizar a API Fake jsonplaceholder ainda. \n",
    "import requests\n",
    "\n",
    "# URL dos posts\n",
    "url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "\n",
    "# Fazendo a requisição GET\n",
    "response = requests.get(url)\n",
    "\n",
    "# Convertendo para JSON (lista de dicionários)\n",
    "posts = response.json()\n",
    "\n",
    "# Filtrando os posts do usuário com userId 5\n",
    "posts_usuario_5 = [post for post in posts if post['userId'] == 5]\n",
    "\n",
    "# Contando quantos posts\n",
    "print(f\"O usuário publicou {len(posts_usuario_5)} posts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2ea9c31f-7ca3-4ac2-b71e-8ee0e822faba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Dúvidas sobre a aceleração na carreira dos servidores técnicos-administrativos da UFTM é tema de audiência pública\n",
      "2. Estação de qualidade do ar é instalada na UFTM\n",
      "3. Medicina alcança nota 5 na renovação do reconhecimento do curso\n",
      "4. O primeiro encontro do curso Contratações Públicas: do planejamento à fiscalização foi realizado no última terça-feira\n",
      "5. Escolas de Uberaba podem receber o Planetário Itinerante da UFTM\n",
      "6. Grupo de pesquisa da pós-graduação em Educação Física apresenta trabalhos em congresso brasileiro\n"
     ]
    }
   ],
   "source": [
    "#Beautiful Soup é usada para fazer web scraping, ou seja, para extrair informações de páginas web de forma automática. Com a Beautiful \n",
    "#Soup, você pode navegar e buscar elementos dentro do código HTML de uma página, como títulos, links, textos, tabelas, entre outros, \n",
    "#facilitando a coleta de dados que não estão disponíveis em APIs ou bancos de dados. Por exemplo, você pode usar a Beautiful Soup para\n",
    "#extrair todos os títulos de notícias de um site de jornal, ou coletar preços e descrições de produtos de uma loja virtual. Isso ajuda \n",
    "#bastante em projetos de análise de dados quando os dados precisam ser coletados diretamente da web. Se quiser, posso mostrar um exemplo \n",
    "#simples de como usar a Beautiful Soup para pegar os títulos das notícias de uma página!\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL da página que queremos extrair os dados\n",
    "url = \"https://www.uftm.edu.br/\"\n",
    "\n",
    "# Fazendo a requisição HTTP para pegar o conteúdo da página\n",
    "response = requests.get(url)\n",
    "\n",
    "# Criando o objeto BeautifulSoup para analisar o HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Encontrando todos os elementos de título de notícia (exemplo: tags <h3>)\n",
    "titulos = soup.find_all('h3')\n",
    "\n",
    "# Imprimindo os títulos encontrados\n",
    "for i, titulo in enumerate(titulos, start=1):\n",
    "    print(f\"{i}. {titulo.get_text(strip=True)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
